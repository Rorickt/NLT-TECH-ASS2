{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09726366-86d7-412c-9dcd-d1666d0e5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all functions for feature extraction\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_features_and_labels(trainingfile):\n",
    "    \"\"\"\n",
    "    trainingfile: preprocessed conllu file\n",
    "\n",
    "    \"\"\"\n",
    "    predicate_per_sent=[]\n",
    "    with open(trainingfile, encoding='utf8') as file:\n",
    "        file = csv.reader(file, delimiter='\\t', quotechar='^')\n",
    "        predicates=[]\n",
    "        for row in file:\n",
    "            # if current row is empty, it marks a sentence boundary\n",
    "            if row == []:\n",
    "                predicates.append('_') # add a empty predicate to sentences without predicate\n",
    "                predicate_per_sent.append(predicates)\n",
    "                predicates=[]\n",
    "            \n",
    "            # if token is a predicate: append to list of predicates of current sentence\n",
    "            elif row !=[] and not row[0].startswith(\"#\") and not row[0].startswith('\"'):\n",
    "                if row[10] != '_':\n",
    "                    predicate = row[10]\n",
    "                    predicates.append(predicate[:-3])\n",
    "                    \n",
    "    with open(trainingfile, encoding='utf8') as file:\n",
    "        file = csv.reader(file, delimiter='\\t', quotechar='^')\n",
    "        features = []\n",
    "        labels = []\n",
    "        tokens = []\n",
    "        prev_token = ''\n",
    "        prev_pos = ''\n",
    "        prev_deprel = ''\n",
    "        \n",
    "        current_sent = 0\n",
    "        for row in file:\n",
    "              \n",
    "            if row == []:\n",
    "                prev_token = ''\n",
    "                prev_pos = ''\n",
    "                prev_deprel = ''\n",
    "            elif row != [] and row[0].startswith(\"#\"):\n",
    "                continue\n",
    "            elif row != [] and not row[0].startswith(\"#\") and not row[0].startswith('\"'):\n",
    "                token_labels = []\n",
    "                token_features = {}\n",
    "                current_token = row[1]\n",
    "                current_pos = row[3]\n",
    "                token_preds = []\n",
    "                if prev_pos == \"AUX\":\n",
    "                    passive = 1\n",
    "                else:\n",
    "                    passive = 0\n",
    "                deprel_to_head = row[7] # dependency relation\n",
    "                if row[6] > row[0]: # if current is before head\n",
    "                    head_init = 0\n",
    "                elif row[6] < row[0]:\n",
    "                    head_init = 1\n",
    "                else:\n",
    "                    head_init = 2\n",
    "                for i, element in enumerate(row):\n",
    "                    if i > 10:\n",
    "                        column_num = i -11\n",
    "                        if element == '_':\n",
    "                            head_pred = '_'\n",
    "                            deprel_to_head = '_'\n",
    "                        else:\n",
    "                            head_pred = predicate_per_sent[current_sent][column_num]\n",
    "\n",
    "                        token_preds.append(head_pred)\n",
    "                        token_labels.append(element)\n",
    "                \n",
    "                        token_features = {'token':current_token, 'pos':current_pos, \n",
    "                                'prev_token':prev_token, 'prev_pos':prev_pos, \n",
    "                                'head_pred':head_pred, 'deprel_to_head':deprel_to_head,\n",
    "                                'prev_deprel':prev_deprel, 'head_init':head_init, \n",
    "                                'passive':passive}\n",
    "\n",
    "                # reassign prev token after adding to feature list\n",
    "                prev_token = row[1]\n",
    "                prev_pos = row[3]\n",
    "                prev_deprel = deprel_to_head\n",
    "            \n",
    "                labels.append(element)\n",
    "                features.append(token_features)\n",
    "\n",
    "    return features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db11fd1-d3e5-4535-b7f9-f686c5e20d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, training_labels = extract_features_and_labels(\"../data/en_ewt-up-train_preprocessed.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef464ff2-54c1-4bb8-9661-c6f6bce6a88f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': 'Al', 'pos': 'PROPN', 'prev_token': '', 'prev_pos': '', 'head_pred': '_', 'deprel_to_head': '_', 'prev_deprel': '', 'head_init': 1, 'passive': 0}, {'token': '-', 'pos': 'PUNCT', 'prev_token': 'Al', 'prev_pos': 'PROPN', 'head_pred': '_', 'deprel_to_head': '_', 'prev_deprel': '_', 'head_init': 1, 'passive': 0}, {'token': 'Zaman', 'pos': 'PROPN', 'prev_token': '-', 'prev_pos': 'PUNCT', 'head_pred': '_', 'deprel_to_head': '_', 'prev_deprel': '_', 'head_init': 1, 'passive': 0}, {'token': ':', 'pos': 'PUNCT', 'prev_token': 'Zaman', 'prev_pos': 'PROPN', 'head_pred': '_', 'deprel_to_head': '_', 'prev_deprel': '_', 'head_init': 1, 'passive': 0}, {'token': 'American', 'pos': 'ADJ', 'prev_token': ':', 'prev_pos': 'PUNCT', 'head_pred': '_', 'deprel_to_head': '_', 'prev_deprel': '_', 'head_init': 0, 'passive': 0}, {'token': 'forces', 'pos': 'NOUN', 'prev_token': 'American', 'prev_pos': 'ADJ', 'head_pred': 'k', 'deprel_to_head': 'nsubj', 'prev_deprel': '_', 'head_init': 0, 'passive': 0}, {'token': 'killed', 'pos': 'VERB', 'prev_token': 'forces', 'prev_pos': 'NOUN', 'head_pred': 'k', 'deprel_to_head': 'parataxis', 'prev_deprel': 'nsubj', 'head_init': 1, 'passive': 0}, {'token': 'Shaikh', 'pos': 'PROPN', 'prev_token': 'killed', 'prev_pos': 'VERB', 'head_pred': 'k', 'deprel_to_head': 'obj', 'prev_deprel': 'parataxis', 'head_init': 1, 'passive': 0}, {'token': 'Abdullah', 'pos': 'PROPN', 'prev_token': 'Shaikh', 'prev_pos': 'PROPN', 'head_pred': '_', 'deprel_to_head': '_', 'prev_deprel': 'obj', 'head_init': 1, 'passive': 0}, {'token': 'al', 'pos': 'PROPN', 'prev_token': 'Abdullah', 'prev_pos': 'PROPN', 'head_pred': '_', 'deprel_to_head': '_', 'prev_deprel': '_', 'head_init': 0, 'passive': 0}]\n"
     ]
    }
   ],
   "source": [
    "print(training_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea14977e-0640-441c-a44f-b0043ee38e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '_', '_', '_', '_', 'ARG0', 'V', 'ARG1', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "print(training_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfac90a0-f07c-4c51-a541-dfb4181a5289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features_args(dev_test_data):\n",
    "       \n",
    "    predicate_per_sent=[]\n",
    "    with open(dev_test_data, encoding='utf8') as file:\n",
    "        file = csv.reader(file, delimiter='\\t', quotechar='^')\n",
    "        predicates=[]\n",
    "        for row in file:\n",
    "            # if current row is empty, it marks a sentence boundary\n",
    "            if row == []:\n",
    "                predicates.append('_') # add a empty predicate to sentences without predicate\n",
    "                predicate_per_sent.append(predicates)\n",
    "                predicates=[]\n",
    "            \n",
    "            # if token is a predicate: append to list of predicates of current sentence\n",
    "            elif row !=[] and not row[0].startswith(\"#\") and not row[0].startswith('\"'):\n",
    "                if row[10] != '_':\n",
    "                    predicate = row[10]\n",
    "                    predicates.append(predicate[:-3])\n",
    "                    \n",
    "    with open(dev_test_data, encoding='utf8') as file:\n",
    "        file = csv.reader(file, delimiter='\\t', quotechar='^')\n",
    "        features = []\n",
    "        labels = []\n",
    "        tokens = []\n",
    "        prev_token = ''\n",
    "        prev_pos = ''\n",
    "        prev_deprel = ''\n",
    "        \n",
    "        current_sent = 0\n",
    "        for row in file:\n",
    "              \n",
    "            if row == []:\n",
    "                prev_token = ''\n",
    "                prev_pos = ''\n",
    "                prev_deprel = ''\n",
    "            elif row != [] and row[0].startswith(\"#\"):\n",
    "                continue\n",
    "            elif row != [] and not row[0].startswith(\"#\") and not row[0].startswith('\"'):\n",
    "                if row[-1] == \"ARG\":\n",
    "                    token_labels = []\n",
    "                    token_features = {}\n",
    "                    current_token = row[1]\n",
    "                    current_pos = row[3]\n",
    "                    token_preds = []\n",
    "                    if prev_pos == \"AUX\":\n",
    "                        passive = 1\n",
    "                    else:\n",
    "                        passive = 0\n",
    "                    deprel_to_head = row[7] # dependency relation\n",
    "                    if row[6] > row[0]: # if current is before head\n",
    "                        head_init = 0\n",
    "                    elif row[6] < row[0]:\n",
    "                        head_init = 1\n",
    "                    else:\n",
    "                        head_init = 2\n",
    "                    for i, element in enumerate(row):\n",
    "                        if i > 10:\n",
    "                            column_num = i -11\n",
    "                            if element == '_':\n",
    "                                head_pred = '_'\n",
    "                                deprel_to_head = '_'\n",
    "                            else:\n",
    "                                head_pred = predicate_per_sent[current_sent][column_num]\n",
    "\n",
    "                            token_preds.append(head_pred)\n",
    "                            token_labels.append(element)\n",
    "\n",
    "                            token_features = {'token':current_token, 'pos':current_pos, \n",
    "                                    'prev_token':prev_token, 'prev_pos':prev_pos, \n",
    "                                    'head_pred':head_pred, 'deprel_to_head':deprel_to_head,\n",
    "                                    'prev_deprel':prev_deprel, 'head_init':head_init, \n",
    "                                    'passive':passive}\n",
    "\n",
    "                    # reassign prev token after adding to feature list\n",
    "                    prev_token = row[1]\n",
    "                    prev_pos = row[3]\n",
    "                    prev_deprel = deprel_to_head\n",
    "\n",
    "                    labels.append(element)\n",
    "                    features.append(token_features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a3aa97-b1c0-4cd7-9f23-47e03678ea4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_test_features, lab = extract_features_and_labels(\"en_ewt-up-dev_preprocessed_argclass.conllu\")\n",
    "# print(dev_test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebf3a33a-7d89-4853-be68-a96aea5028c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_classifier(training_features, training_labels, modelname):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    modeltype = LogisticRegression(max_iter = 400)\n",
    "        \n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(training_features)\n",
    "    model = modeltype.fit(features_vectorized, training_labels)\n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "826dc86b-0482-4e2d-b9ec-7d1d20e118b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mojca\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg_features, logreg_vec= create_classifier(training_features, training_labels, \"logreg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a37914cd-6cef-46a1-84ee-db10ff8f1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, inputdata, outputfile, vec = None):\n",
    "    \"\"\"\n",
    "    Use a trained classifier to classify inputdata and write the predictions to a file. Uses extract_features\n",
    "    \n",
    "    model: the trained classifier\n",
    "    type model:\n",
    "    vec: the vectorizer used to transform data\n",
    "    type vec:\n",
    "    inputdata: path to file with input data\n",
    "    type inputdata: string\n",
    "    outputfile: path to file where output should be stored\n",
    "    type outputfile: string\n",
    "    \n",
    "    return: None\n",
    "\"\"\"\n",
    "    features = inputdata\n",
    "    features = vec.transform(features)\n",
    "    predictions = model.predict(features)\n",
    "  \n",
    "    predlist = []\n",
    "    for item in predictions:\n",
    "        predlist.append(item)\n",
    "    return predlist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40e82563-c3aa-42c6-a6df-90a610198279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predlist = classify_data(logreg_features, dev_test_features, \"trial_output.tsv\", logreg_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8b9f456-5a03-41d1-be24-96cb8bd9b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenlist = []\n",
    "arglist = []\n",
    "counter = 0\n",
    "with open(\"en_ewt-up-dev_preprocessed.conllu\", \"r\", encoding = \"utf8\") as inf:\n",
    "    infile = csv.reader(inf, delimiter='\\t', quotechar='*')\n",
    "    for row in infile:\n",
    "        if row != [] and not row[0].startswith(\"#\") and not row[0].startswith('\"'):\n",
    "            tokenlist.append(row[1])\n",
    "            arglist.append(row[-1])\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ae84172-1801-4a11-8f4d-778323f48edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mojca\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4883005772562087, 0.4521671853759478, 0.45015327876051126, None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(arglist, predlist, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324643b0-d375-4b3d-bddd-83d81b7a1e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
